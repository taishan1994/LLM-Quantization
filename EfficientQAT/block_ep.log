nohup: ignoring input
['main_block_ap.py', '--model', '/data/gongoubo/checkpoints/Qwen/Qwen3-4B', '--output_dir', './output/block_ap_log/qwen3-4b-w4g128', '--wbits', '4', '--group_size', '128', '--quant_lr', '1e-4', '--weight_lr', '1e-5', '--save_quant_dir', './output/block_ap_models/qwen3-4b-w4g128', '--real_quant']
[2025-08-30 16:28:54 root](main_block_ap.py 115): INFO Namespace(model='/data/gongoubo/checkpoints/Qwen/Qwen3-4B', cache_dir='./cache', output_dir='./output/block_ap_log/qwen3-4b-w4g128', save_quant_dir='./output/block_ap_models/qwen3-4b-w4g128', real_quant=True, resume_quant=None, calib_dataset='redpajama', train_size=4096, val_size=64, training_seqlen=2048, batch_size=2, epochs=2, ppl_seqlen=2048, seed=2, eval_ppl=False, eval_tasks='', eval_batch_size=16, wbits=4, group_size=128, quant_lr=0.0001, weight_lr=1e-05, min_lr_factor=20, clip_grad=0.3, wd=0, net=None, max_memory='70GiB', early_stop=0, off_load_to_disk=False)
[2025-08-30 16:28:54 root](main_block_ap.py 119): INFO net is None, setting as Qwen3-4B
/data/gongoubo/checkpoints/Qwen/Qwen3-4B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.54it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.59it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.80it/s]
[2025-08-30 16:28:56 root](main_block_ap.py 135): INFO === start quantization ===
[2025-08-30 16:28:56 root](main_block_ap.py 142): INFO load trainloader from ./cache/dataloader_Qwen3-4B_redpajama_4096_64_2048_train.cache
[2025-08-30 16:28:56 root](main_block_ap.py 144): INFO load valloader from ./cache/dataloader_Qwen3-4B_redpajama_4096_64_2048_val.cache
[2025-08-30 16:28:56 root](block_ap.py 46): INFO Starting ...
[2025-08-30 16:29:11 root](block_ap.py 141): INFO The caught attention_mask is None. Proceeding without an explicit attention mask.
[2025-08-30 16:29:14 root](block_ap.py 181): INFO === Start quantize blocks 0===
trainable parameter number: 102.507776M
[2025-08-30 16:32:17 root](block_ap.py 303): INFO blocks 0 epoch 0 recon_loss:0.00012454617535695434 val_loss:0.00012495231931097806 quant_lr:5.246359588146619e-05 norm:0.00084549 max memory_allocated 3831.68798828125 time 144.68319511413574 
[2025-08-30 16:34:43 root](block_ap.py 303): INFO blocks 0 epoch 1 recon_loss:0.00010414076677989215 val_loss:0.00010619576642056927 quant_lr:5e-06 norm:0.00072435 max memory_allocated 3871.68798828125 time 146.77786111831665 
[2025-08-30 16:35:21 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 16:35:22 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 16:35:22 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 16:35:22 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 16:35:23 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 16:35:24 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 16:35:25 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 16:35:26 root](block_ap.py 181): INFO === Start quantize blocks 1===
trainable parameter number: 102.507776M
[2025-08-30 16:38:30 root](block_ap.py 303): INFO blocks 1 epoch 0 recon_loss:0.00019661114492919296 val_loss:0.00020226291962899268 quant_lr:5.246359588146619e-05 norm:0.00396264 max memory_allocated 3871.68798828125 time 146.6197111606598 
[2025-08-30 16:40:57 root](block_ap.py 303): INFO blocks 1 epoch 1 recon_loss:0.00018058020214084536 val_loss:0.00018831469060387462 quant_lr:5e-06 norm:0.00284996 max memory_allocated 3871.68798828125 time 147.04443860054016 
[2025-08-30 16:41:37 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 16:41:37 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 16:41:37 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 16:41:38 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 16:41:39 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 16:41:40 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 16:41:41 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 16:41:41 root](block_ap.py 181): INFO === Start quantize blocks 2===
trainable parameter number: 102.507776M
[2025-08-30 16:44:48 root](block_ap.py 303): INFO blocks 2 epoch 0 recon_loss:0.00026268328656442463 val_loss:0.0002702134079299867 quant_lr:5.246359588146619e-05 norm:0.00270974 max memory_allocated 3871.68798828125 time 146.82968497276306 
[2025-08-30 16:47:15 root](block_ap.py 303): INFO blocks 2 epoch 1 recon_loss:0.00024265007232315838 val_loss:0.00025180267402902246 quant_lr:5e-06 norm:0.00208261 max memory_allocated 3872.18798828125 time 147.45982956886292 
[2025-08-30 16:47:56 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 16:47:56 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 16:47:56 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 16:47:57 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 16:47:58 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 16:47:59 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 16:48:00 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 16:48:00 root](block_ap.py 181): INFO === Start quantize blocks 3===
trainable parameter number: 102.507776M
[2025-08-30 16:51:05 root](block_ap.py 303): INFO blocks 3 epoch 0 recon_loss:0.0005274466820992529 val_loss:0.0005097499815747142 quant_lr:5.246359588146619e-05 norm:0.00455027 max memory_allocated 3872.18798828125 time 147.31425619125366 
[2025-08-30 16:53:31 root](block_ap.py 303): INFO blocks 3 epoch 1 recon_loss:0.00045748482807539403 val_loss:0.0004751328087877482 quant_lr:5e-06 norm:0.00481066 max memory_allocated 3872.18798828125 time 146.31733894348145 
[2025-08-30 16:54:08 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 16:54:08 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 16:54:08 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 16:54:09 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 16:54:10 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 16:54:11 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 16:54:12 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 16:54:12 root](block_ap.py 181): INFO === Start quantize blocks 4===
trainable parameter number: 102.507776M
[2025-08-30 16:57:17 root](block_ap.py 303): INFO blocks 4 epoch 0 recon_loss:0.0009785934817045927 val_loss:0.0009826058521866798 quant_lr:5.246359588146619e-05 norm:0.00631002 max memory_allocated 3872.18798828125 time 147.09790062904358 
[2025-08-30 16:59:43 root](block_ap.py 303): INFO blocks 4 epoch 1 recon_loss:0.0008753198781050742 val_loss:0.0009061401942744851 quant_lr:5e-06 norm:0.00505723 max memory_allocated 3872.18798828125 time 146.56280779838562 
[2025-08-30 17:00:22 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:00:22 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:00:22 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:00:23 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:00:24 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:00:24 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:00:26 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:00:26 root](block_ap.py 181): INFO === Start quantize blocks 5===
trainable parameter number: 102.507776M
[2025-08-30 17:03:29 root](block_ap.py 303): INFO blocks 5 epoch 0 recon_loss:0.0017321869963780046 val_loss:0.0017650588415563107 quant_lr:5.246359588146619e-05 norm:0.00437822 max memory_allocated 3872.18798828125 time 146.4142291545868 
[2025-08-30 17:05:56 root](block_ap.py 303): INFO blocks 5 epoch 1 recon_loss:0.0015733613399788737 val_loss:0.0016322677256539464 quant_lr:5e-06 norm:0.00362037 max memory_allocated 3872.18798828125 time 147.32654762268066 
[2025-08-30 17:06:38 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:06:38 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:06:38 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:06:38 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:06:39 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:06:40 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:06:42 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:06:42 root](block_ap.py 181): INFO === Start quantize blocks 6===
trainable parameter number: 102.507776M
[2025-08-30 17:09:44 root](block_ap.py 303): INFO blocks 6 epoch 0 recon_loss:0.007037381175905466 val_loss:0.0059229666367173195 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 146.729510307312 
[2025-08-30 17:12:12 root](block_ap.py 303): INFO blocks 6 epoch 1 recon_loss:0.00421329028904438 val_loss:0.005103854462504387 quant_lr:5e-06 norm:0.72215718 max memory_allocated 3872.18798828125 time 147.16277980804443 
[2025-08-30 17:12:49 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:12:49 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:12:49 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:12:50 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:12:51 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:12:52 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:12:53 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:12:54 root](block_ap.py 181): INFO === Start quantize blocks 7===
trainable parameter number: 102.507776M
[2025-08-30 17:15:58 root](block_ap.py 303): INFO blocks 7 epoch 0 recon_loss:0.005626752506941557 val_loss:0.006775266490876675 quant_lr:5.246359588146619e-05 norm:0.01033138 max memory_allocated 3872.18798828125 time 147.40487265586853 
[2025-08-30 17:18:25 root](block_ap.py 303): INFO blocks 7 epoch 1 recon_loss:0.0053118192590773106 val_loss:0.0065146274864673615 quant_lr:5e-06 norm:0.00882644 max memory_allocated 3872.18798828125 time 146.63793301582336 
[2025-08-30 17:19:02 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:19:02 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:19:02 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:19:03 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:19:04 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:19:05 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:19:06 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:19:06 root](block_ap.py 181): INFO === Start quantize blocks 8===
trainable parameter number: 102.507776M
[2025-08-30 17:22:14 root](block_ap.py 303): INFO blocks 8 epoch 0 recon_loss:0.007282709237188101 val_loss:0.00852860976010561 quant_lr:5.246359588146619e-05 norm:0.01204145 max memory_allocated 3872.18798828125 time 146.1728572845459 
[2025-08-30 17:24:41 root](block_ap.py 303): INFO blocks 8 epoch 1 recon_loss:0.0068693314678967 val_loss:0.008184104226529598 quant_lr:5e-06 norm:0.00983516 max memory_allocated 3872.18798828125 time 147.66877460479736 
[2025-08-30 17:25:20 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:25:20 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:25:20 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:25:20 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:25:21 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:25:22 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:25:24 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:25:24 root](block_ap.py 181): INFO === Start quantize blocks 9===
trainable parameter number: 102.507776M
[2025-08-30 17:28:26 root](block_ap.py 303): INFO blocks 9 epoch 0 recon_loss:0.007952150888741016 val_loss:0.009221675805747509 quant_lr:5.246359588146619e-05 norm:0.02226457 max memory_allocated 3872.18798828125 time 146.39471912384033 
[2025-08-30 17:30:54 root](block_ap.py 303): INFO blocks 9 epoch 1 recon_loss:0.007489942014217377 val_loss:0.008845845237374306 quant_lr:5e-06 norm:0.01632103 max memory_allocated 3872.18798828125 time 147.4408619403839 
[2025-08-30 17:31:30 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:31:30 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:31:30 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:31:31 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:31:32 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:31:33 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:31:34 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:31:34 root](block_ap.py 181): INFO === Start quantize blocks 10===
trainable parameter number: 102.507776M
[2025-08-30 17:34:39 root](block_ap.py 303): INFO blocks 10 epoch 0 recon_loss:0.007857666350901127 val_loss:0.009085224941372871 quant_lr:5.246359588146619e-05 norm:0.02380521 max memory_allocated 3872.18798828125 time 147.28605031967163 
[2025-08-30 17:37:05 root](block_ap.py 303): INFO blocks 10 epoch 1 recon_loss:0.007241277489811182 val_loss:0.008584544062614441 quant_lr:5e-06 norm:0.01819926 max memory_allocated 3872.18798828125 time 146.39890789985657 
[2025-08-30 17:37:46 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:37:46 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:37:46 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:37:47 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:37:47 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:37:48 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:37:50 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:37:50 root](block_ap.py 181): INFO === Start quantize blocks 11===
trainable parameter number: 102.507776M
[2025-08-30 17:40:58 root](block_ap.py 303): INFO blocks 11 epoch 0 recon_loss:0.00831123162060976 val_loss:0.00964342150837183 quant_lr:5.246359588146619e-05 norm:0.01824945 max memory_allocated 3872.18798828125 time 147.0486123561859 
[2025-08-30 17:43:25 root](block_ap.py 303): INFO blocks 11 epoch 1 recon_loss:0.007833309471607208 val_loss:0.009282514452934265 quant_lr:5e-06 norm:0.01330457 max memory_allocated 3872.18798828125 time 147.04177904129028 
[2025-08-30 17:44:06 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:44:06 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:44:06 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:44:07 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:44:08 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:44:08 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:44:10 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:44:10 root](block_ap.py 181): INFO === Start quantize blocks 12===
trainable parameter number: 102.507776M
[2025-08-30 17:47:13 root](block_ap.py 303): INFO blocks 12 epoch 0 recon_loss:0.00888146460056305 val_loss:0.010337134823203087 quant_lr:5.246359588146619e-05 norm:0.01839816 max memory_allocated 3872.18798828125 time 146.48548030853271 
[2025-08-30 17:49:40 root](block_ap.py 303): INFO blocks 12 epoch 1 recon_loss:0.008430328220129013 val_loss:0.0099778538569808 quant_lr:5e-06 norm:0.01362778 max memory_allocated 3872.18798828125 time 147.36506175994873 
[2025-08-30 17:50:17 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:50:17 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:50:18 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:50:18 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:50:19 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:50:22 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:50:23 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:50:23 root](block_ap.py 181): INFO === Start quantize blocks 13===
trainable parameter number: 102.507776M
[2025-08-30 17:53:29 root](block_ap.py 303): INFO blocks 13 epoch 0 recon_loss:0.01009887270629406 val_loss:0.011713181622326374 quant_lr:5.246359588146619e-05 norm:0.02293788 max memory_allocated 3872.18798828125 time 148.4098560810089 
[2025-08-30 17:55:56 root](block_ap.py 303): INFO blocks 13 epoch 1 recon_loss:0.009525277651846409 val_loss:0.011245793662965298 quant_lr:5e-06 norm:0.01422751 max memory_allocated 3872.18798828125 time 146.71751165390015 
[2025-08-30 17:56:36 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 17:56:36 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 17:56:36 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 17:56:37 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 17:56:37 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 17:56:38 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 17:56:40 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 17:56:40 root](block_ap.py 181): INFO === Start quantize blocks 14===
trainable parameter number: 102.507776M
[2025-08-30 17:59:46 root](block_ap.py 303): INFO blocks 14 epoch 0 recon_loss:0.010478869080543518 val_loss:0.012133198790252209 quant_lr:5.246359588146619e-05 norm:0.01930365 max memory_allocated 3872.18798828125 time 147.45297646522522 
[2025-08-30 18:02:13 root](block_ap.py 303): INFO blocks 14 epoch 1 recon_loss:0.009856805205345154 val_loss:0.011605584993958473 quant_lr:5e-06 norm:0.01398535 max memory_allocated 3872.18798828125 time 146.59228444099426 
[2025-08-30 18:02:55 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:02:56 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:02:56 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:02:57 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:02:57 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:02:58 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:03:00 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:03:00 root](block_ap.py 181): INFO === Start quantize blocks 15===
trainable parameter number: 102.507776M
[2025-08-30 18:06:03 root](block_ap.py 303): INFO blocks 15 epoch 0 recon_loss:0.01156130712479353 val_loss:0.013424847275018692 quant_lr:5.246359588146619e-05 norm:0.02200064 max memory_allocated 3872.18798828125 time 146.2896978855133 
[2025-08-30 18:08:30 root](block_ap.py 303): INFO blocks 15 epoch 1 recon_loss:0.011068159714341164 val_loss:0.012982962653040886 quant_lr:5e-06 norm:0.01646485 max memory_allocated 3872.18798828125 time 147.14335322380066 
[2025-08-30 18:09:08 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:09:08 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:09:08 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:09:08 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:09:09 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:09:10 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:09:12 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:09:12 root](block_ap.py 181): INFO === Start quantize blocks 16===
trainable parameter number: 102.507776M
[2025-08-30 18:12:17 root](block_ap.py 303): INFO blocks 16 epoch 0 recon_loss:2.385732650756836 val_loss:3.1108317375183105 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 146.80607748031616 
[2025-08-30 18:14:44 root](block_ap.py 303): INFO blocks 16 epoch 1 recon_loss:2.1329803466796875 val_loss:2.935401201248169 quant_lr:5e-06 norm:39.75217056 max memory_allocated 3872.18798828125 time 147.17392563819885 
[2025-08-30 18:15:22 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:15:22 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:15:23 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:15:23 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:15:24 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:15:25 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:15:26 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:15:26 root](block_ap.py 181): INFO === Start quantize blocks 17===
trainable parameter number: 102.507776M
[2025-08-30 18:18:34 root](block_ap.py 303): INFO blocks 17 epoch 0 recon_loss:2.0935451984405518 val_loss:2.963578462600708 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 146.70323133468628 
[2025-08-30 18:21:02 root](block_ap.py 303): INFO blocks 17 epoch 1 recon_loss:2.0848748683929443 val_loss:2.958695888519287 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 147.6906270980835 
[2025-08-30 18:21:43 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:21:43 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:21:43 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:21:44 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:21:45 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:21:46 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:21:47 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:21:47 root](block_ap.py 181): INFO === Start quantize blocks 18===
trainable parameter number: 102.507776M
[2025-08-30 18:24:57 root](block_ap.py 303): INFO blocks 18 epoch 0 recon_loss:2.08768367767334 val_loss:2.937471389770508 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.65734887123108 
[2025-08-30 18:27:28 root](block_ap.py 303): INFO blocks 18 epoch 1 recon_loss:2.1023342609405518 val_loss:2.9185657501220703 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 150.73162722587585 
[2025-08-30 18:28:05 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:28:05 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:28:06 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:28:06 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:28:07 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:28:08 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:28:09 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:28:09 root](block_ap.py 181): INFO === Start quantize blocks 19===
trainable parameter number: 102.507776M
[2025-08-30 18:31:13 root](block_ap.py 303): INFO blocks 19 epoch 0 recon_loss:2.1039340496063232 val_loss:2.9242756366729736 quant_lr:5.246359588146619e-05 norm:0.08662893 max memory_allocated 3872.18798828125 time 146.75233006477356 
[2025-08-30 18:33:40 root](block_ap.py 303): INFO blocks 19 epoch 1 recon_loss:2.1033806800842285 val_loss:2.9256539344787598 quant_lr:5e-06 norm:0.15377393 max memory_allocated 3872.18798828125 time 147.24788188934326 
[2025-08-30 18:34:17 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:34:17 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:34:17 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:34:18 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:34:19 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:34:20 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:34:21 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:34:21 root](block_ap.py 181): INFO === Start quantize blocks 20===
trainable parameter number: 102.507776M
[2025-08-30 18:37:24 root](block_ap.py 303): INFO blocks 20 epoch 0 recon_loss:2.1061763763427734 val_loss:2.9283814430236816 quant_lr:5.246359588146619e-05 norm:0.10165208 max memory_allocated 3872.18798828125 time 146.54441618919373 
[2025-08-30 18:39:51 root](block_ap.py 303): INFO blocks 20 epoch 1 recon_loss:2.104175329208374 val_loss:2.9281935691833496 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.63891172409058 
[2025-08-30 18:40:32 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:40:32 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:40:33 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:40:33 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:40:34 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:40:35 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:40:36 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:40:36 root](block_ap.py 181): INFO === Start quantize blocks 21===
trainable parameter number: 102.507776M
[2025-08-30 18:43:48 root](block_ap.py 303): INFO blocks 21 epoch 0 recon_loss:2.1120007038116455 val_loss:2.9365251064300537 quant_lr:5.246359588146619e-05 norm:0.12277831 max memory_allocated 3872.18798828125 time 151.268940448761 
[2025-08-30 18:46:15 root](block_ap.py 303): INFO blocks 21 epoch 1 recon_loss:2.1098384857177734 val_loss:2.935821771621704 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 147.2970051765442 
[2025-08-30 18:46:57 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:46:57 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:46:57 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:46:58 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:46:59 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:47:00 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:47:01 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:47:01 root](block_ap.py 181): INFO === Start quantize blocks 22===
trainable parameter number: 102.507776M
[2025-08-30 18:50:08 root](block_ap.py 303): INFO blocks 22 epoch 0 recon_loss:2.115518093109131 val_loss:2.942580223083496 quant_lr:5.246359588146619e-05 norm:0.22490773 max memory_allocated 3872.18798828125 time 147.48099160194397 
[2025-08-30 18:52:38 root](block_ap.py 303): INFO blocks 22 epoch 1 recon_loss:2.1123499870300293 val_loss:2.943997383117676 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 149.80189990997314 
[2025-08-30 18:53:15 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:53:15 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:53:15 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:53:16 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:53:17 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:53:17 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:53:19 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:53:19 root](block_ap.py 181): INFO === Start quantize blocks 23===
trainable parameter number: 102.507776M
[2025-08-30 18:56:24 root](block_ap.py 303): INFO blocks 23 epoch 0 recon_loss:2.1292035579681396 val_loss:2.9623775482177734 quant_lr:5.246359588146619e-05 norm:0.18271481 max memory_allocated 3872.18798828125 time 147.27022671699524 
[2025-08-30 18:58:51 root](block_ap.py 303): INFO blocks 23 epoch 1 recon_loss:2.1257643699645996 val_loss:2.9619321823120117 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.9681704044342 
[2025-08-30 18:59:29 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 18:59:29 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 18:59:29 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 18:59:30 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 18:59:31 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 18:59:31 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 18:59:33 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 18:59:33 root](block_ap.py 181): INFO === Start quantize blocks 24===
trainable parameter number: 102.507776M
[2025-08-30 19:02:38 root](block_ap.py 303): INFO blocks 24 epoch 0 recon_loss:2.1494691371917725 val_loss:2.9859631061553955 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.0728189945221 
[2025-08-30 19:05:05 root](block_ap.py 303): INFO blocks 24 epoch 1 recon_loss:2.144749164581299 val_loss:2.984504222869873 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.98867106437683 
[2025-08-30 19:05:41 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:05:41 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:05:42 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:05:42 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:05:43 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:05:44 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:05:46 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:05:46 root](block_ap.py 181): INFO === Start quantize blocks 25===
trainable parameter number: 102.507776M
[2025-08-30 19:08:51 root](block_ap.py 303): INFO blocks 25 epoch 0 recon_loss:2.165938377380371 val_loss:3.0059027671813965 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.21627283096313 
[2025-08-30 19:11:18 root](block_ap.py 303): INFO blocks 25 epoch 1 recon_loss:2.1616528034210205 val_loss:3.0037410259246826 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 147.1476194858551 
[2025-08-30 19:11:55 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:11:55 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:11:56 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:11:56 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:11:57 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:11:58 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:11:59 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:11:59 root](block_ap.py 181): INFO === Start quantize blocks 26===
trainable parameter number: 102.507776M
[2025-08-30 19:15:04 root](block_ap.py 303): INFO blocks 26 epoch 0 recon_loss:2.1895816326141357 val_loss:3.0323526859283447 quant_lr:5.246359588146619e-05 norm:0.17389622 max memory_allocated 3872.18798828125 time 147.61589932441711 
[2025-08-30 19:17:31 root](block_ap.py 303): INFO blocks 26 epoch 1 recon_loss:2.185887098312378 val_loss:3.030705690383911 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.78978776931763 
[2025-08-30 19:18:09 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:18:09 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:18:09 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:18:09 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:18:10 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:18:11 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:18:13 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:18:13 root](block_ap.py 181): INFO === Start quantize blocks 27===
trainable parameter number: 102.507776M
[2025-08-30 19:21:16 root](block_ap.py 303): INFO blocks 27 epoch 0 recon_loss:2.231654167175293 val_loss:3.0782480239868164 quant_lr:5.246359588146619e-05 norm:0.25724813 max memory_allocated 3872.18798828125 time 146.7264654636383 
[2025-08-30 19:23:44 root](block_ap.py 303): INFO blocks 27 epoch 1 recon_loss:2.226597547531128 val_loss:3.0746328830718994 quant_lr:5e-06 norm:0.21364649 max memory_allocated 3872.18798828125 time 147.8051209449768 
[2025-08-30 19:24:21 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:24:21 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:24:22 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:24:22 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:24:23 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:24:24 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:24:25 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:24:25 root](block_ap.py 181): INFO === Start quantize blocks 28===
trainable parameter number: 102.507776M
[2025-08-30 19:27:30 root](block_ap.py 303): INFO blocks 28 epoch 0 recon_loss:2.2919089794158936 val_loss:3.1420884132385254 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.2272560596466 
[2025-08-30 19:29:57 root](block_ap.py 303): INFO blocks 28 epoch 1 recon_loss:2.284947395324707 val_loss:3.1373891830444336 quant_lr:5e-06 norm:0.34809271 max memory_allocated 3872.18798828125 time 146.92916250228882 
[2025-08-30 19:30:34 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:30:34 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:30:34 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:30:34 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:30:35 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:30:36 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:30:38 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:30:38 root](block_ap.py 181): INFO === Start quantize blocks 29===
trainable parameter number: 102.507776M
[2025-08-30 19:33:43 root](block_ap.py 303): INFO blocks 29 epoch 0 recon_loss:2.3645379543304443 val_loss:3.2200942039489746 quant_lr:5.246359588146619e-05 norm:0.34402186 max memory_allocated 3872.18798828125 time 147.15573382377625 
[2025-08-30 19:36:10 root](block_ap.py 303): INFO blocks 29 epoch 1 recon_loss:2.356558322906494 val_loss:3.2152624130249023 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.79655408859253 
[2025-08-30 19:36:46 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:36:46 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:36:46 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:36:47 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:36:48 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:36:49 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:36:50 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:36:50 root](block_ap.py 181): INFO === Start quantize blocks 30===
trainable parameter number: 102.507776M
[2025-08-30 19:39:55 root](block_ap.py 303): INFO blocks 30 epoch 0 recon_loss:2.4845495223999023 val_loss:3.3499794006347656 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.23839020729065 
[2025-08-30 19:42:22 root](block_ap.py 303): INFO blocks 30 epoch 1 recon_loss:2.4738175868988037 val_loss:3.342381238937378 quant_lr:5e-06 norm:0.51511407 max memory_allocated 3872.18798828125 time 146.95932745933533 
[2025-08-30 19:43:00 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:43:00 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:43:00 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:43:01 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:43:02 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:43:03 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:43:04 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:43:04 root](block_ap.py 181): INFO === Start quantize blocks 31===
trainable parameter number: 102.507776M
[2025-08-30 19:46:15 root](block_ap.py 303): INFO blocks 31 epoch 0 recon_loss:2.6245815753936768 val_loss:3.4992194175720215 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 148.21589064598083 
[2025-08-30 19:48:42 root](block_ap.py 303): INFO blocks 31 epoch 1 recon_loss:2.610152006149292 val_loss:3.4904096126556396 quant_lr:5e-06 norm:0.56064349 max memory_allocated 3872.18798828125 time 147.14544534683228 
[2025-08-30 19:49:22 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:49:22 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:49:22 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:49:23 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:49:24 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:49:25 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:49:26 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:49:26 root](block_ap.py 181): INFO === Start quantize blocks 32===
trainable parameter number: 102.507776M
[2025-08-30 19:52:31 root](block_ap.py 303): INFO blocks 32 epoch 0 recon_loss:2.8144516944885254 val_loss:3.701052665710449 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.0910451412201 
[2025-08-30 19:54:58 root](block_ap.py 303): INFO blocks 32 epoch 1 recon_loss:2.796038866043091 val_loss:3.6890039443969727 quant_lr:5e-06 norm:0.89721912 max memory_allocated 3872.18798828125 time 147.1864366531372 
[2025-08-30 19:55:36 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 19:55:36 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 19:55:36 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 19:55:36 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 19:55:37 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 19:55:38 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 19:55:40 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 19:55:40 root](block_ap.py 181): INFO === Start quantize blocks 33===
trainable parameter number: 102.507776M
[2025-08-30 19:58:45 root](block_ap.py 303): INFO blocks 33 epoch 0 recon_loss:3.0794477462768555 val_loss:3.9790406227111816 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.2818808555603 
[2025-08-30 20:01:12 root](block_ap.py 303): INFO blocks 33 epoch 1 recon_loss:3.0512003898620605 val_loss:3.960819721221924 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.80788707733154 
[2025-08-30 20:01:50 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 20:01:50 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 20:01:51 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 20:01:51 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 20:01:52 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 20:01:53 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 20:01:54 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 20:01:54 root](block_ap.py 181): INFO === Start quantize blocks 34===
trainable parameter number: 102.507776M
[2025-08-30 20:04:59 root](block_ap.py 303): INFO blocks 34 epoch 0 recon_loss:1.5189248323440552 val_loss:1.634688138961792 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 147.12826085090637 
[2025-08-30 20:07:26 root](block_ap.py 303): INFO blocks 34 epoch 1 recon_loss:1.4738187789916992 val_loss:1.6025910377502441 quant_lr:5e-06 norm:nan max memory_allocated 3872.18798828125 time 146.9902367591858 
[2025-08-30 20:08:05 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 20:08:05 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 20:08:05 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 20:08:05 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 20:08:06 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 20:08:07 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 20:08:08 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 20:08:08 root](block_ap.py 181): INFO === Start quantize blocks 35===
trainable parameter number: 102.507776M
[2025-08-30 20:11:14 root](block_ap.py 303): INFO blocks 35 epoch 0 recon_loss:2.2516181468963623 val_loss:2.388124942779541 quant_lr:5.246359588146619e-05 norm:nan max memory_allocated 3872.18798828125 time 146.85207056999207 
[2025-08-30 20:13:41 root](block_ap.py 303): INFO blocks 35 epoch 1 recon_loss:2.139603853225708 val_loss:2.3269591331481934 quant_lr:5e-06 norm:9.40097904 max memory_allocated 3872.18798828125 time 146.7489185333252 
[2025-08-30 20:14:20 root](block_ap.py 340): INFO pack quantized self_attn.q_proj finished
[2025-08-30 20:14:20 root](block_ap.py 340): INFO pack quantized self_attn.k_proj finished
[2025-08-30 20:14:20 root](block_ap.py 340): INFO pack quantized self_attn.v_proj finished
[2025-08-30 20:14:21 root](block_ap.py 340): INFO pack quantized self_attn.o_proj finished
[2025-08-30 20:14:22 root](block_ap.py 340): INFO pack quantized mlp.gate_proj finished
[2025-08-30 20:14:23 root](block_ap.py 340): INFO pack quantized mlp.up_proj finished
[2025-08-30 20:14:24 root](block_ap.py 340): INFO pack quantized mlp.down_proj finished
[2025-08-30 20:14:28 root](main_block_ap.py 163): INFO 13532.3022711277
[2025-08-30 20:14:28 root](main_block_ap.py 166): INFO start saving model
[2025-08-30 20:14:31 root](main_block_ap.py 169): INFO save model success
