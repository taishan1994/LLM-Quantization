base:
    seed: &seed 42
model:
    type: Llama
    path: /mnt/nvme1/yongyang/models/llama2-7b
    torch_dtype: auto
# eval:
#     eval_pos: [pretrain, fake_quant]
#     name: wikitext2
#     download: False
#     path: /mnt/nvme0/yongyang/llm_datasets/llmc/eval/wikitext2
#     seq_len: 2048
#     # For 7B / 13B model eval, bs can be set to "1", and inference_per_block can be set to "False".
#     # For 70B model eval, bs can be set to "20", and inference_per_block can be set to "True".
#     bs: 1
#     inference_per_block: False
quant:
    method: RTN
    weight:
        quant_type: float-quant
        bit: e4m3
        symmetric: True
        granularity: per_channel
    act:
        quant_type: float-quant
        bit: e4m3
        symmetric: True
        granularity: per_token
save:
    save_fake: False
    save_path: /path/to/save/
